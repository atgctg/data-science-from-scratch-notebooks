{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Data Science from Scratch repository into the folder `dsfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('dsfs'):\n",
    "    !git clone --depth 1 https://github.com/joelgrus/data-science-from-scratch dsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the needed functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsfs.scratch.linear_algebra import Vector, dot, distance, add, scalar_multiply, vector_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Computes the sum of squared elements in v\"\"\"\n",
    "    return dot(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def difference_quotient(f: Callable[[float], float],\n",
    "                        x: float,\n",
    "                        h: float) -> float:\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x: float) -> float:\n",
    "    return x * x\n",
    "\n",
    "def derivative(x: float) -> float:\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = range(-10, 11)\n",
    "actuals = [derivative(x) for x in xs]\n",
    "estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
    "\n",
    "# plot to show they're basically the same\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.plot(xs, actuals, 'rx', label='Actual')       # red  x\n",
    "plt.plot(xs, estimates, 'b+', label='Estimate')   # blue +\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f: Callable[[Vector], float],\n",
    "                                    v: Vector,\n",
    "                                    i: int,\n",
    "                                    h: float) -> float:\n",
    "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n",
    "            for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f: Callable[[Vector], float],\n",
    "                      v: Vector,\n",
    "                      h: float = 0.0001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i in range(len(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0001000000078335, 2.000100000003613]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f(x, y) = x^2 + y^2\n",
    "# fx'(x, y) = 2x\n",
    "# fy'(x, y) = 2y\n",
    "\n",
    "estimate_gradient(lambda vector: vector[0] ** 2 + vector[1] ** 2, [2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-2.2521191009629953, -0.8271312144159854, -3.8043117507360216]\n",
      "100 [-0.29867503499180315, -0.10969377432253591, -0.5045261348678024]\n",
      "200 [-0.039610150497462804, -0.014547539635073988, -0.06691003194346434]\n",
      "300 [-0.005253080567897856, -0.0019292882457648325, -0.008873578721245994]\n",
      "400 [-0.0006966612119939712, -0.0002558613503462995, -0.0011768100692087756]\n",
      "500 [-9.239090054374097e-05, -3.393221865355813e-05, -0.00015606802875094144]\n",
      "600 [-1.225284019882724e-05, -4.500075768358537e-06, -2.0697672662319426e-05]\n",
      "700 [-1.6249662256178467e-06, -5.967980498924495e-07, -2.7449161565317765e-06]\n",
      "800 [-2.1550229918541214e-07, -7.914709233559137e-08, -3.64029561647577e-07]\n",
      "900 [-2.8579819212267575e-08, -1.049645223590715e-08, -4.827743879826332e-08]\n"
     ]
    }
   ],
   "source": [
    "# pick a random starting point\n",
    "v = [random.uniform(-10, 10) for i in range(3)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
    "    v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
    "    if epoch % 100 == 0:                 # print every 100 epochs\n",
    "        print(epoch, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(v, [0, 0, 0]) < 0.001    # v should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gradient Descent to Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x ranges from -50 to 49, y is always 20 * x + 5\n",
    "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
    "\n",
    "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
    "    slope, intercept = theta\n",
    "    predicted = slope * x + intercept    # The prediction of the model.\n",
    "    error = (predicted - y)              # error is (predicted - actual)\n",
    "    squared_error = error ** 2           # We'll minimize squared error\n",
    "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [33.66356019423278, -0.174670657030819]\n",
      "1000 [19.99958064528482, 4.301774147360938]\n",
      "2000 [19.999943325959265, 4.905637688138569]\n",
      "3000 [19.99999234073977, 4.987247327113169]\n",
      "4000 [19.999998964882924, 4.99827652944751]\n"
     ]
    }
   ],
   "source": [
    "# Start with random values for slope and intercept.\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # Compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
    "    # Take a step in that direction\n",
    "    theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, List, Iterator\n",
    "\n",
    "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
    "\n",
    "def minibatches(dataset: List[T],\n",
    "                batch_size: int,\n",
    "                shuffle: bool = True) -> Iterator[List[T]]:\n",
    "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
    "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
    "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
    "\n",
    "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
    "\n",
    "    for start in batch_starts:\n",
    "        end = start + batch_size\n",
    "        yield dataset[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [19.174202715078767, 1.079209387747482]\n",
      "100 [20.267395842970828, 4.330392766687993]\n",
      "200 [19.992748148789925, 4.826898806075673]\n",
      "300 [20.000014117407332, 4.960295982979091]\n",
      "400 [20.001111219955906, 4.99325906073336]\n",
      "500 [20.00000750566774, 4.997630902238474]\n",
      "600 [20.000001793957406, 4.999567727602932]\n",
      "700 [19.99999533067239, 4.99986955748152]\n",
      "800 [19.9999899808177, 4.9999576506544825]\n",
      "900 [20.00000033989906, 4.999992824479073]\n"
     ]
    }
   ],
   "source": [
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for batch in minibatches(inputs, batch_size=20):\n",
    "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [20.091422139670822, 0.44973150232344167]\n",
      "10 [20.0589663750914, 2.0651013388512043]\n",
      "20 [20.03803298265049, 3.1070061736705443]\n",
      "30 [20.024531058911496, 3.779029178507888]\n",
      "40 [20.015822394846502, 4.212480396644925]\n",
      "50 [20.010205352819014, 4.492054097774306]\n",
      "60 [20.006582420397063, 4.672377629459557]\n",
      "70 [20.00424561322211, 4.788685337413492]\n",
      "80 [20.002738406799633, 4.8637031816411636]\n",
      "90 [20.001766253782453, 4.91208928670809]\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient descent example\n",
    "theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in inputs:\n",
    "        grad = linear_gradient(x, y, theta)\n",
    "        theta = gradient_step(theta, grad, -learning_rate)\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, theta)\n",
    "\n",
    "slope, intercept = theta\n",
    "assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < intercept < 5.1, \"intercept should be about 5\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dsfs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65d971c10a713b748fb14366d4f1295f22d91a925f68b8da1d8144bd6b74555a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
